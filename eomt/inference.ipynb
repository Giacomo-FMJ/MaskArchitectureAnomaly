{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© 2025 Mobile Perception Systems Lab at TU/e. All rights reserved. Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T10:24:05.798036Z",
     "start_time": "2025-12-22T10:23:38.513404Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Data/universita/secondoAnno/01_advanced_machine_learning/exam/MaskArchitectureAnomaly/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import yaml\n",
    "import gdown\n",
    "import zipfile\n",
    "import warnings\n",
    "import requests\n",
    "import importlib\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Iterable, Any\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import average_precision_score, roc_curve\n",
    "from dsets.generic_anomaly import GenericAnomalyDataset\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.amp.autocast_mode import autocast\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub.utils import RepositoryNotFoundError\n",
    "\n",
    "from lightning import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T10:24:05.810058Z",
     "start_time": "2025-12-22T10:24:05.800023Z"
    }
   },
   "outputs": [],
   "source": [
    "def fpr_at_95_tpr(scores, labels):\n",
    "    \"\"\"\n",
    "    Calculates the False Positive Rate (FPR) when the True Positive Rate (TPR) is 95%\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    if all(tpr < 0.95):\n",
    "        return 1.0\n",
    "    # Find the index where TPR is at least 0.95\n",
    "    idx = np.argmax(tpr >= 0.95)\n",
    "    return fpr[idx]\n",
    "\n",
    "def select_device(device_: str) -> torch.device:\n",
    "\n",
    "    if device_ == \"auto\":\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        elif torch.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "    if device_ == \"cpu\":\n",
    "        return torch.device(\"cpu\")\n",
    "    elif device_ == \"cuda\" and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif device_ == \"mps\" and torch.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "\n",
    "    print(\"[WARNING] Device not found or not available. Using CPU.\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "def get_datasets(datapath: str = \"\"):\n",
    "    \"\"\"\n",
    "    Downloads the Cityscape datasets into the specified directory if they are not already present.\n",
    "\n",
    "    :param datapath: Path where the datasets will be stored. If not provided, defaults to \"raw_datasets\".\n",
    "    :type datapath: str\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    chunk = 10_000_000 # 10 MB chunk\n",
    "    datapath = datapath or \"raw_datasets\"\n",
    "    os.makedirs(datapath, exist_ok=True)\n",
    "\n",
    "    if len(os.listdir(datapath)):\n",
    "        print(\"Cityscape datasets already downloaded\")\n",
    "        return\n",
    "\n",
    "    s = requests.Session()\n",
    "    s.post(\"https://www.cityscapes-dataset.com/login/\",\n",
    "           data={\"username\":\"AdryG\",\"password\":\"Agf2gc262!\",\"submit\":\"Login\"})\n",
    "\n",
    "    for pid in [1, 3]:\n",
    "        r = s.get(f\"https://www.cityscapes-dataset.com/file-handling/?packageID={pid}\",\n",
    "                  stream=True, allow_redirects=True)\n",
    "\n",
    "        r.raise_for_status()\n",
    "\n",
    "        cd = r.headers.get(\"content-disposition\",\"\")\n",
    "        fname = cd.split(\"filename=\")[-1].strip('\"') if \"filename=\" in cd else f\"{pid}.zip\"\n",
    "        size = int(r.headers.get(\"content-length\", 0))\n",
    "\n",
    "        with open(f\"raw_datasets/{fname}\", \"wb\") as f, tqdm(total=size, unit=\"B\", unit_scale=True, desc=fname) as p:\n",
    "            for c in r.iter_content(chunk):\n",
    "                f.write(c); p.update(len(c))\n",
    "\n",
    "\n",
    "def get_eomt_trained_model():\n",
    "    \"\"\"\n",
    "    Downloads the EOMT trained model from the given Google Drive folder link, extracts its contents,\n",
    "    and performs cleanup and renaming operations if necessary.\n",
    "\n",
    "    The function checks if the local folder `eomt_trained_model` exists. If not, it downloads the files\n",
    "    from the provided URL using the `gdown` library. After downloading the folder, it looks for a zip\n",
    "    file within the folder, unzips its contents, and skips files related to `macosx`. Additionally, if\n",
    "    the dataset name contains a typo (e.g., \"Obsticle\"), it renames it to correct the spelling\n",
    "    (e.g., \"Obstacle\").\n",
    "\n",
    "    :raises zipfile.BadZipFile: If the zip file within the folder is corrupted or invalid.\n",
    "    :raises OSError: If there is an issue accessing or modifying files or directories.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    # folder link\n",
    "    url = \"https://drive.google.com/drive/folders/1q2vHUzora2nP52fP50zmoQAykWuwoGav?usp=share_link\"\n",
    "\n",
    "    folder_name = \"eomt_trained_model\"\n",
    "\n",
    "    if not os.path.exists(folder_name):\n",
    "        gdown.download_folder(url, output=folder_name, quiet=True)\n",
    "\n",
    "    zip_path = os.path.join(folder_name, \"validation_Dataset.zip\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_obj:\n",
    "        for file_name in zip_obj.namelist():\n",
    "            if \"macosx\" in file_name.lower():\n",
    "                continue\n",
    "            zip_obj.extract(file_name, folder_name)\n",
    "\n",
    "    # fix a typo in the dataset name\n",
    "    for dirname in os.listdir(os.path.join(folder_name, \"Validation_Dataset\")):\n",
    "        if \"obsticle\" in dirname.lower():\n",
    "            os.rename(os.path.join(folder_name, \"Validation_Dataset\", dirname), os.path.join(folder_name, \"Validation_Dataset\", dirname.replace(\"Obsticle\", \"Obstacle\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T10:48:57.265127Z",
     "start_time": "2025-12-22T10:24:05.812589Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gtFine_trainvaltest.zip: 100%|██████████| 253M/253M [00:03<00:00, 68.2MB/s] \n",
      "leftImg8bit_trainvaltest.zip: 100%|██████████| 11.6G/11.6G [24:14<00:00, 7.97MB/s]  \n"
     ]
    }
   ],
   "source": [
    "get_datasets()\n",
    "get_eomt_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T11:11:36.973868Z",
     "start_time": "2025-12-22T11:11:36.926768Z"
    }
   },
   "outputs": [],
   "source": [
    "seed_everything(0, verbose=False)\n",
    "\n",
    "device = select_device(\"auto\")\n",
    "img_idx = 2  # change to the index of the image you want to visualize\n",
    "config_path = \"configs/dinov2/cityscapes/semantic/eomt_base_640.yaml\"\n",
    "data_path = \"raw_datasets\"  # dataset directory\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "def create_mapping(images, ignore_index):\n",
    "    unique_ids = np.unique(np.concatenate([np.unique(img) for img in images]))\n",
    "    valid_ids = unique_ids[unique_ids != ignore_index]\n",
    "    colors = np.array(\n",
    "        [plt.cm.hsv(i / len(valid_ids))[:3] for i in range(len(valid_ids))]\n",
    "    )\n",
    "    mapping = {cid: colors[i] for i, cid in enumerate(valid_ids)}\n",
    "    mapping[ignore_index] = np.array([0, 0, 0])\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def apply_colormap(image, mapping):\n",
    "    colored_image = np.zeros((*image.shape, 3))\n",
    "    for cid in np.unique(image):\n",
    "        colored_image[image == cid] = mapping.get(cid, [0, 0, 0])\n",
    "    return colored_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Ensure the dataset files are correctly prepared and placed in the folder specified by `data_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T11:11:38.388128Z",
     "start_time": "2025-12-22T11:11:37.942155Z"
    }
   },
   "outputs": [],
   "source": [
    "data_module_name, class_name = config[\"data\"][\"class_path\"].rsplit(\".\", 1)\n",
    "data_module = getattr(importlib.import_module(data_module_name), class_name)\n",
    "data_module_kwargs = config[\"data\"].get(\"init_args\", {})\n",
    "\n",
    "data = data_module(\n",
    "    path=data_path,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    check_empty_targets=False,\n",
    "    **data_module_kwargs\n",
    ").setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T11:11:43.137667Z",
     "start_time": "2025-12-22T11:11:39.043386Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*Attribute 'network' is an instance of `nn\\.Module` and is already saved during checkpointing.*\",\n",
    ")\n",
    "\n",
    "# Load encoder\n",
    "encoder_cfg = config[\"model\"][\"init_args\"][\"network\"][\"init_args\"][\"encoder\"]\n",
    "encoder_module_name, encoder_class_name = encoder_cfg[\"class_path\"].rsplit(\".\", 1)\n",
    "encoder_cls = getattr(importlib.import_module(encoder_module_name), encoder_class_name)\n",
    "encoder = encoder_cls(img_size=data.img_size, **encoder_cfg.get(\"init_args\", {}))\n",
    "\n",
    "# Load network\n",
    "network_cfg = config[\"model\"][\"init_args\"][\"network\"]\n",
    "network_module_name, network_class_name = network_cfg[\"class_path\"].rsplit(\".\", 1)\n",
    "network_cls = getattr(importlib.import_module(network_module_name), network_class_name)\n",
    "network_kwargs = {k: v for k, v in network_cfg[\"init_args\"].items() if k != \"encoder\"}\n",
    "network = network_cls(\n",
    "    masked_attn_enabled=False,\n",
    "    num_classes=data.num_classes,\n",
    "    encoder=encoder,\n",
    "    **network_kwargs,\n",
    ")\n",
    "\n",
    "# Load Lightning module\n",
    "lit_module_name, lit_class_name = config[\"model\"][\"class_path\"].rsplit(\".\", 1)\n",
    "lit_cls = getattr(importlib.import_module(lit_module_name), lit_class_name)\n",
    "model_kwargs = {k: v for k, v in config[\"model\"][\"init_args\"].items() if k != \"network\"}\n",
    "if \"stuff_classes\" in config[\"data\"].get(\"init_args\", {}):\n",
    "    model_kwargs[\"stuff_classes\"] = config[\"data\"][\"init_args\"][\"stuff_classes\"]\n",
    "\n",
    "model = (\n",
    "    lit_cls(\n",
    "        img_size=data.img_size,\n",
    "        num_classes=data.num_classes,\n",
    "        network=network,\n",
    "        **model_kwargs,\n",
    "    )\n",
    "    .eval()\n",
    "    .to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained weights from Hugging Face Hub\n",
    "The model weights are downloaded from the Hugging Face Hub using the logger name from the config. Make sure you have a working internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T11:11:43.499541Z",
     "start_time": "2025-12-22T11:11:43.474150Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_pretrained_model(model_,\n",
    "                          device_,\n",
    "                          *,\n",
    "                          config_=None,\n",
    "                          local_path=None,\n",
    "                          repo_namespace=\"tue-mps\",\n",
    "                          hf_filename=\"pytorch_model.bin\",\n",
    "                          strict=False,\n",
    "                          map_location=None,\n",
    "                          ):\n",
    "\n",
    "    \"\"\"\n",
    "    Load a pretrained model with optional configurations and specified parameters.\n",
    "\n",
    "    This function attempts to load a pretrained model's weights either from a\n",
    "    local path or from the HuggingFace Hub. If a configuration object with logger\n",
    "    information is provided, it attempts to locate and download the model from\n",
    "    the specified HuggingFace repository. If a mismatch in the positional\n",
    "    embedding shapes between the model and checkpoint is detected, the positional\n",
    "    embedding in the state dictionary will be ignored. After loading the state\n",
    "    dictionary, the model is set to evaluation mode and moved to the specified\n",
    "    device.\n",
    "\n",
    "    :param model_: The PyTorch model to load the pretrained weights into.\n",
    "    :param device_: The device to move the model to after loading the weights.\n",
    "    :param config_: Optional configuration object containing model and logger settings.\n",
    "    :param local_path: Optional local file path to a pretrained checkpoint.\n",
    "    :param repo_namespace: Namespace of the HuggingFace Hub repository where the pretrained model is hosted. Defaults to \"tue-mps\".\n",
    "    :param hf_filename: Filename of the pretrained model checkpoint in the repository. Defaults to \"pytorch_model.bin\".\n",
    "    :param strict: Whether to enforce strict loading of layers into the model. Defaults to False.\n",
    "    :param map_location: Device mapping location for loading the checkpoint. Defaults to None.\n",
    "    :return: The model with loaded pretrained weights, set to evaluation mode and moved to the specified device.\n",
    "    \"\"\"\n",
    "\n",
    "    ckpt_path = local_path or None\n",
    "\n",
    "    if config_:\n",
    "        name_ = (config_.get(\"trainer\", {})\n",
    "                .get(\"logger\", {})\n",
    "                .get(\"init_args\", {})\n",
    "                .get(\"name\"))\n",
    "\n",
    "        if name_:\n",
    "            try:\n",
    "                ckpt_path = hf_hub_download(\n",
    "                    repo_id=f\"{repo_namespace}/{name_}\",\n",
    "                    filename=hf_filename)\n",
    "            except RepositoryNotFoundError:\n",
    "                warnings.warn(f\"Pretrained model `{name_}` not found on HuggingFace Hub\")\n",
    "                return model_.eval().to(device_)\n",
    "\n",
    "    if not ckpt_path:\n",
    "        warnings.warn(\"No checkpoint provided or found\")\n",
    "        return model_.eval().to(device_)\n",
    "\n",
    "    ckpt = torch.load(ckpt_path, map_location=map_location)\n",
    "\n",
    "    # Lightning .ckpt\n",
    "    state_dict_ = ckpt[\"state_dict\"] if isinstance(ckpt, dict) and \"state_dict\" in ckpt else ckpt\n",
    "\n",
    "    # Drop positional embedding if shape mismatches\n",
    "    for key in (\"network.encoder.backbone.pos_embed\", \"encoder.backbone.pos_embed\"):\n",
    "        if key in state_dict_ and key in model_.state_dict():\n",
    "            if state_dict_[key].shape != model_.state_dict()[key].shape:\n",
    "                del state_dict_[key]\n",
    "\n",
    "    model_.load_state_dict(state_dict_, strict=strict)\n",
    "    model_.eval().to(device_)\n",
    "    return model_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T11:11:44.844862Z",
     "start_time": "2025-12-22T11:11:44.273245Z"
    }
   },
   "outputs": [],
   "source": [
    "# eomt (via config)\n",
    "# model = load_pretrained_model(\n",
    "#     model,\n",
    "#     device,\n",
    "#     config_=config,\n",
    "#     repo_namespace=\"tue-mps\",\n",
    "#     hf_filename=\"pytorch_model.bin\",\n",
    "#     map_location=device,\n",
    "#     strict=False)\n",
    "\n",
    "# eomt (da .ckpt locale)\n",
    "model = load_pretrained_model(\n",
    "    model,\n",
    "    device,\n",
    "    local_path=\"eomt_trained_model/epoch_106-step_19902_eomt.ckpt\",\n",
    "    map_location=device,\n",
    "    strict=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic inference (pixel-wise classification)\n",
    "\n",
    "> This inference method also works when applied to a model trained for panoptic segmentation.\n",
    "\n",
    "Semantic inference computes per-pixel class scores by combining mask and class predictions:\n",
    "\n",
    "$$\n",
    "\\sum_i p_i(c) \\cdot m_i[h, w]\n",
    "$$\n",
    "\n",
    "Here, $p_i(c)$ is the class probability for class $c$ (excluding \"no object\"), and $m_i[h, w]$ is the sigmoid-normalized mask value for query $i$ at pixel $(h, w)$. The final class is selected by taking the argmax over classes.\n",
    "\n",
    "*This inference method was originally introduced in MaskFormer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T11:11:46.531505Z",
     "start_time": "2025-12-22T11:11:46.505577Z"
    }
   },
   "outputs": [],
   "source": [
    "IGNORE_INDEX = 255\n",
    "DEVICE_TYPE = select_device(\"auto\").type\n",
    "\n",
    "def infer_semantic(img, target):\n",
    "    with torch.no_grad(), autocast(dtype=torch.float16, device_type=DEVICE_TYPE):\n",
    "        imgs = [img.to(device)]\n",
    "        img_sizes = [img.shape[-2:] for img in imgs]\n",
    "        crops, origins = model.window_imgs_semantic(imgs)\n",
    "\n",
    "        mask_logits_per_layer, class_logits_per_layer = model(crops)\n",
    "        mask_logits = F.interpolate(\n",
    "            mask_logits_per_layer[-1], data.img_size, mode=\"bilinear\"\n",
    "        )\n",
    "\n",
    "        crop_logits = model.to_per_pixel_logits_semantic(\n",
    "            mask_logits, class_logits_per_layer[-1]\n",
    "        )\n",
    "        logits = model.revert_window_logits_semantic(crop_logits, origins, img_sizes)\n",
    "        preds = logits[0].argmax(0).cpu()\n",
    "\n",
    "    pred_array = preds.numpy()\n",
    "    target_array = model.to_per_pixel_targets_semantic([target], IGNORE_INDEX)[0].numpy()\n",
    "    return logits[0], pred_array, target_array\n",
    "\n",
    "\n",
    "def plot_semantic_results(img, pred_array, target_array):\n",
    "    mapping = create_mapping([pred_array, target_array], IGNORE_INDEX)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5), tight_layout=True)\n",
    "    axes[0].imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "    axes[0].set_title(\"Image\")\n",
    "    axes[1].imshow(apply_colormap(pred_array, mapping))\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "    axes[2].imshow(apply_colormap(target_array, mapping))\n",
    "    axes[2].set_title(\"Target\")\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# img, target = data.val_dataloader().dataset[img_idx]\n",
    "# pred_array, target_array = infer_semantic(img, target)\n",
    "# plot_semantic_results(img, pred_array, target_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T15:37:46.744629Z",
     "start_time": "2025-12-22T15:37:46.724635Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_anomaly_maps(logits_chw: torch.Tensor, T: float) -> dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute pixel-wise anomaly score maps from logits [C,H,W] for a given temperature T\n",
    "    \"\"\"\n",
    "\n",
    "    z = logits_chw / T\n",
    "    probs = F.softmax(z, dim=0)\n",
    "\n",
    "    # MSP\n",
    "    msp = 1. - probs.max(dim=0).values\n",
    "\n",
    "    # MaxLogit\n",
    "    maxlogit = -z.max(dim=0).values\n",
    "\n",
    "    # Entropy\n",
    "    eps = 1e-12\n",
    "    entropy = -torch.sum(probs * torch.log(probs + eps), dim=0)\n",
    "\n",
    "    return {\"MSP\": msp, \"MaxLogit\": maxlogit, \"Entropy\": entropy}\n",
    "\n",
    "\n",
    "def eval_one_dataset(\n",
    "        *,\n",
    "        dataset_name: str,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        Ts: Iterable[float],\n",
    "        ignore_index: int = IGNORE_INDEX) -> list[dict[str, Any]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Evaluates a semantic segmentation model on a single dataset and computes performance metrics for\n",
    "    anomaly detection such as Area under Precision-Recall Curve (AuPRC) and the False Positive Rate at\n",
    "    95% True Positive Rate (FPR95). The evaluation process involves anomaly mapping computation, label\n",
    "    binarization, and metric aggregation across multiple threshold temperatures.\n",
    "\n",
    "    :param dataset_name: The name of the dataset being evaluated.\n",
    "    :type dataset_name: str\n",
    "    :param dataloader: An object providing batches of images and their associated targets for evaluation.\n",
    "    :type dataloader: torch.utils.data.DataLoader\n",
    "    :param Ts: A collection of temperature scaling values to be applied during the computation of anomaly maps.\n",
    "    :type Ts: Iterable[float]\n",
    "    :param ignore_index: Labels marked with this index are ignored during evaluation to avoid invalid pixels.\n",
    "    :type ignore_index: int\n",
    "    :return: A list of dictionaries containing evaluation results for each threshold temperature and method,\n",
    "             including the AuPRC and FPR95 metrics for the given dataset and configuration.\n",
    "    :rtype: list[dict[str, Any]]\n",
    "    \"\"\"\n",
    "\n",
    "    methods = (\"MSP\", \"MaxLogit\", \"Entropy\")\n",
    "\n",
    "    acc_labels: list[np.ndarray] = []\n",
    "    acc_scores: dict[tuple[str, float], list[np.ndarray]] = {(m, t): [] for m in methods for t in Ts}\n",
    "\n",
    "    for _img, _target in tqdm(dataloader, desc=f\"Evaluating {dataset_name:>20s}\"):\n",
    "        # If DataLoader adds a batch dimension (B, C, H, W), keep only the first sample (we evaluate one image at a time)\n",
    "        if isinstance(_img, torch.Tensor) and _img.dim() == 4:\n",
    "            _img = _img[0]\n",
    "\n",
    "        # If the target is a dict (e.g., {\"masks\": ..., \"labels\": ...}), DataLoader collates each value and adds a batch dim,\n",
    "        # so we remove that leading batch dimension from every tensor in the dict\n",
    "        if isinstance(_target, dict):\n",
    "            _target = {k:(v[0] if isinstance(v, torch.Tensor) else v) for k,v in _target.items()}\n",
    "\n",
    "        # Otherwise, if the target is a tensor with shape (B, H, W), remove the batch dimension (keep the first sample).\n",
    "        elif isinstance(_target, torch.Tensor) and _target.dim() == 3:\n",
    "            _target = _target[0]\n",
    "\n",
    "        logits_chw, _, target_array = infer_semantic(_img, _target)\n",
    "        logits_cpu = logits_chw.detach().to(\"cpu\", dtype=torch.float32)\n",
    "        gt = target_array\n",
    "        valid = (gt != ignore_index)\n",
    "\n",
    "        if not np.any(valid): continue\n",
    "\n",
    "        # binarize labels\n",
    "        y = (gt[valid] == 1).astype(np.uint8)\n",
    "        acc_labels.append(y)\n",
    "\n",
    "        # compute a score map for each T and store only valid pixels\n",
    "        for t in Ts:\n",
    "            maps = compute_anomaly_maps(logits_cpu, t)\n",
    "            for m in methods:\n",
    "                score = maps[m].numpy()[valid].astype(np.float32)\n",
    "                acc_scores[(m,t)].append(score)\n",
    "\n",
    "    if not len(acc_labels):\n",
    "        warnings.warn(f\"No valid pixels found for dataset `{dataset_name}`\")\n",
    "        return []\n",
    "\n",
    "    y_true = np.concatenate(acc_labels, axis=0)\n",
    "\n",
    "    rows: list[dict[str, Any]] = []\n",
    "    for t in Ts:\n",
    "        for m in methods:\n",
    "            scores = np.concatenate(acc_scores[(m,t)], axis=0)\n",
    "            auprc = round(average_precision_score(y_true, scores) * 100., 2)\n",
    "            fpr95 = round(fpr_at_95_tpr(scores, y_true) * 100., 2)\n",
    "\n",
    "            rows.append({\n",
    "               \"dataset\": dataset_name,\n",
    "               \"method\": m,\n",
    "               \"T\": t,\n",
    "               \"AuPRC\": auprc,\n",
    "               \"FPR95\": fpr95})\n",
    "\n",
    "    return rows\n",
    "\n",
    "def run_all_datasets(\n",
    "        *,\n",
    "        dataset_cfg: list[dict[str, Any]],\n",
    "        Ts: Iterable[float],\n",
    "        img_size: tuple[int, int],\n",
    "        batch_size: int,\n",
    "        num_workers: int,\n",
    "        out_csv: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Executes evaluation of multiple datasets and aggregates results into a specified output file.\n",
    "\n",
    "    This function iterates through a list of dataset configurations, creates datasets and\n",
    "    data loaders for each configuration, evaluates them, and writes the aggregated results\n",
    "    to a CSV file. Each dataset is evaluated using a specified list of temperature scaling\n",
    "    values, and the results include metrics such as AuPRC and FPR95.\n",
    "\n",
    "    :param dataset_cfg: List of dataset configurations, where each entry specifies\n",
    "        the configuration for a particular dataset.\n",
    "    :param Ts: Iterable of temperature scaling values to be used during evaluation.\n",
    "    :param img_size: Image size as a tuple of integers (height, width).\n",
    "    :param batch_size: Number of samples per batch.\n",
    "    :param num_workers: Number of subprocesses to use for data loading.\n",
    "    :param out_csv: Path to the output CSV file where aggregated results will be saved.\n",
    "    :return: Path to the output CSV file containing the evaluation results.\n",
    "    \"\"\"\n",
    "\n",
    "    all_rows: list[dict[str, Any]] = []\n",
    "\n",
    "    for ds_cfg in dataset_cfg:\n",
    "        ds_name = ds_cfg['name']\n",
    "        ds = GenericAnomalyDataset(ds_cfg, img_size=img_size)\n",
    "        dl = torch.utils.data.DataLoader(ds,\n",
    "                                         batch_size=batch_size,\n",
    "                                         num_workers=num_workers)\n",
    "\n",
    "        rows = eval_one_dataset(\n",
    "            dataset_name=ds_name,\n",
    "            dataloader=dl,\n",
    "            Ts=Ts,\n",
    "            ignore_index=IGNORE_INDEX)\n",
    "\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
    "\n",
    "    with open(out_csv, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"dataset\", \"method\", \"T\", \"AuPRC\", \"FPR95\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_rows)\n",
    "\n",
    "    return out_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T14:13:58.407426Z",
     "start_time": "2025-12-22T14:12:26.592907Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating          RoadAnomaly: 100%|██████████| 60/60 [01:05<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_table5_csv():\n",
    "\n",
    "    # Table 5: same temperature for all methods\n",
    "    Ts = [1]\n",
    "\n",
    "    anomaly_config_path = \"configs/dinov2/common/generic_anomaly.yaml\"\n",
    "\n",
    "    with open(anomaly_config_path, \"r\") as f:\n",
    "        anomaly_cfg = yaml.safe_load(f)\n",
    "\n",
    "    datasets_cfg = anomaly_cfg[\"data\"][\"init_args\"][\"datasets\"]\n",
    "    img_size = tuple(anomaly_cfg[\"data\"][\"init_args\"][\"img_size\"])\n",
    "    num_workers = int(anomaly_cfg[\"data\"][\"init_args\"][\"num_workers\"])\n",
    "    out_csv = \"results/table5.csv\"\n",
    "\n",
    "    out_csv = run_all_datasets(\n",
    "        dataset_cfg=datasets_cfg,\n",
    "        Ts=Ts,\n",
    "        img_size=img_size,\n",
    "        batch_size=1,\n",
    "        num_workers=num_workers,\n",
    "        out_csv=out_csv)\n",
    "\n",
    "get_table5_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T15:37:46.640073Z",
     "start_time": "2025-12-22T15:15:31.025027Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating          RoadAnomaly: 100%|██████████| 60/60 [01:51<00:00,  1.86s/it]\n",
      "Evaluating        RoadAnomaly21: 100%|██████████| 10/10 [00:37<00:00,  3.73s/it]\n",
      "Evaluating       RoadObstacle21: 100%|██████████| 30/30 [01:05<00:00,  2.18s/it]\n",
      "Evaluating    FS_LostFound_full: 100%|██████████| 100/100 [02:46<00:00,  1.67s/it]\n",
      "Evaluating            FS_Static: 100%|██████████| 30/30 [01:09<00:00,  2.32s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_table6_csv(Ts: list[float] | None = None):\n",
    "    # Table 6: test different temperatures\n",
    "    Ts = Ts or [0.25, 0.33, 0.5, 0.67, 0.75, 1.0, 1.25, 1.5, 2.0, 3.0, 4.0, 5.0, 8.0]\n",
    "\n",
    "    anomaly_config_path = \"configs/dinov2/common/generic_anomaly.yaml\"\n",
    "\n",
    "    with open(anomaly_config_path, \"r\") as f:\n",
    "        anomaly_cfg = yaml.safe_load(f)\n",
    "\n",
    "    datasets_cfg = anomaly_cfg[\"data\"][\"init_args\"][\"datasets\"]\n",
    "    img_size = tuple(anomaly_cfg[\"data\"][\"init_args\"][\"img_size\"])\n",
    "    num_workers = int(anomaly_cfg[\"data\"][\"init_args\"][\"num_workers\"])\n",
    "    out_csv = \"results/table6.csv\"\n",
    "\n",
    "    out_csv = run_all_datasets(\n",
    "        dataset_cfg=datasets_cfg,\n",
    "        Ts=Ts,\n",
    "        img_size=img_size,\n",
    "        batch_size=1,\n",
    "        num_workers=num_workers,\n",
    "        out_csv=out_csv)\n",
    "\n",
    "get_table6_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panoptic inference (segmentation with instance IDs)\n",
    "\n",
    "> This inference method also works when applied to a model trained for instance segmentation.\n",
    "\n",
    "Panoptic inference assigns each pixel $[h, w]$ to the query $i$ that maximizes the product of class and mask confidence:\n",
    "\n",
    "$$\n",
    "p_i(c_i) \\cdot m_i[h, w]\n",
    "$$\n",
    "\n",
    "where $c_i = \\arg\\max_c \\, p_i(c)$ is the most likely class for query $i$. A pixel is assigned to a query only if both the class confidence and mask confidence are high. Pixels assigned to the same query form a segment labeled with $c_i$. \"Stuff\" segments with the same class are merged; \"thing\" segments are kept distinct using the query index. Low-confidence and heavily occluded predictions are filtered out.  \n",
    "  \n",
    "*This inference method was originally introduced in MaskFormer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def infer_panoptic(img, target):\n",
    "#     with torch.no_grad(), autocast(dtype=torch.float16, device_type=device):\n",
    "#         imgs = [img.to(device)]\n",
    "#         img_sizes = [img.shape[-2:] for img in imgs]\n",
    "#\n",
    "#         transformed_imgs = model.resize_and_pad_imgs_instance_panoptic(imgs)\n",
    "#         mask_logits_per_layer, class_logits_per_layer = model(transformed_imgs)\n",
    "#         mask_logits = F.interpolate(\n",
    "#             mask_logits_per_layer[-1], model.img_size, mode=\"bilinear\"\n",
    "#         )\n",
    "#         mask_logits = model.revert_resize_and_pad_logits_instance_panoptic(\n",
    "#             mask_logits, img_sizes\n",
    "#         )\n",
    "#\n",
    "#         preds = model.to_per_pixel_preds_panoptic(\n",
    "#             mask_logits,\n",
    "#             class_logits_per_layer[-1],\n",
    "#             model.stuff_classes,\n",
    "#             model.mask_thresh,\n",
    "#             model.overlap_thresh,\n",
    "#         )[0].cpu()\n",
    "#\n",
    "#     pred = preds.numpy()\n",
    "#     sem_pred, inst_pred = pred[..., 0], pred[..., 1]\n",
    "#\n",
    "#     target_seg = model.to_per_pixel_targets_panoptic([target])[0].cpu().numpy()\n",
    "#     sem_target, inst_target = target_seg[..., 0], target_seg[..., 1]\n",
    "#\n",
    "#     return sem_pred, inst_pred, sem_target, inst_target\n",
    "#\n",
    "#\n",
    "# def draw_black_border(sem, inst, mapping):\n",
    "#     h, w = sem.shape\n",
    "#     out = np.zeros((h, w, 3))\n",
    "#     for s in np.unique(sem):\n",
    "#         out[sem == s] = mapping[s]\n",
    "#\n",
    "#     combined = sem.astype(np.int64) * 100000 + inst.astype(np.int64)\n",
    "#     border = np.zeros((h, w), dtype=bool)\n",
    "#     border[1:, :] |= combined[1:, :] != combined[:-1, :]\n",
    "#     border[:-1, :] |= combined[1:, :] != combined[:-1, :]\n",
    "#     border[:, 1:] |= combined[:, 1:] != combined[:, :-1]\n",
    "#     border[:, :-1] |= combined[:, 1:] != combined[:, :-1]\n",
    "#     out[border] = 0\n",
    "#     return out\n",
    "#\n",
    "#\n",
    "# def plot_panoptic_results(img, sem_pred, inst_pred, sem_target, inst_target):\n",
    "#     all_ids = np.union1d(np.unique(sem_pred), np.unique(sem_target))\n",
    "#     mapping = {\n",
    "#         s: (\n",
    "#             [0, 0, 0]\n",
    "#             if s == -1 or s == model.num_classes\n",
    "#             else plt.cm.hsv(i / len(all_ids))[:3]\n",
    "#         )\n",
    "#         for i, s in enumerate(all_ids)\n",
    "#     }\n",
    "#\n",
    "#     vis_pred = draw_black_border(sem_pred, inst_pred, mapping)\n",
    "#     vis_target = draw_black_border(sem_target, inst_target, mapping)\n",
    "#\n",
    "#     img_np = (\n",
    "#         img.cpu().numpy().transpose(1, 2, 0) if img.dim() == 3 else img.cpu().numpy()\n",
    "#     )\n",
    "#\n",
    "#     fig, axes = plt.subplots(1, 3, figsize=(15, 5), tight_layout=True)\n",
    "#     axes[0].imshow(img_np)\n",
    "#     axes[0].set_title(\"Input\")\n",
    "#     axes[1].imshow(vis_pred)\n",
    "#     axes[1].set_title(\"Prediction\")\n",
    "#     axes[2].imshow(vis_target)\n",
    "#     axes[2].set_title(\"Target\")\n",
    "#\n",
    "#     for ax in axes:\n",
    "#         ax.axis(\"off\")\n",
    "#\n",
    "#     plt.show()\n",
    "#\n",
    "#\n",
    "# img, target = data.val_dataloader().dataset[img_idx]\n",
    "# sem_pred, inst_pred, sem_target, inst_target = infer_panoptic(img, target)\n",
    "# plot_panoptic_results(img, sem_pred, inst_pred, sem_target, inst_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
