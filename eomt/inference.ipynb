{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â© 2025 Mobile Perception Systems Lab at TU/e. All rights reserved. Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "import yaml\n",
    "import gdown\n",
    "import zipfile\n",
    "import warnings\n",
    "import requests\n",
    "import importlib\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Iterable, Any\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from datasets.generic_anomaly import GenericAnomalyDataset\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.amp.autocast_mode import autocast\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub.utils import RepositoryNotFoundError\n",
    "\n",
    "from lightning import seed_everything\n",
    "from ood_metrics import fpr_at_95_tpr"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T18:38:03.216527Z",
     "start_time": "2025-12-19T18:38:03.211841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def select_device(device_: str) -> torch.device:\n",
    "\n",
    "    if device_ == \"auto\":\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        elif torch.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "    if device_ == \"cpu\":\n",
    "        return torch.device(\"cpu\")\n",
    "    elif device_ == \"cuda\" and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif device_ == \"mps\" and torch.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "\n",
    "    print(\"[WARNING] Device not found or not available. Using CPU.\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "def get_datasets(datapath: str = \"\"):\n",
    "\n",
    "    chunk = 1024 ** 2 # 1 MB chunk\n",
    "    datapath = datapath or \"raw_datasets\"\n",
    "    os.makedirs(datapath, exist_ok=True)\n",
    "\n",
    "    if len(os.listdir(datapath)):\n",
    "        print(\"Cityscape datasets already downloaded\")\n",
    "        return\n",
    "\n",
    "    s = requests.Session()\n",
    "    s.post(\"https://www.cityscapes-dataset.com/login/\",\n",
    "           data={\"username\":\"AdryG\",\"password\":\"Agf2gc262!\",\"submit\":\"Login\"})\n",
    "\n",
    "    for pid in [1, 3]:\n",
    "        r = s.get(f\"https://www.cityscapes-dataset.com/file-handling/?packageID={pid}\",\n",
    "                  stream=True, allow_redirects=True)\n",
    "\n",
    "        r.raise_for_status()\n",
    "\n",
    "        cd = r.headers.get(\"content-disposition\",\"\")\n",
    "        fname = cd.split(\"filename=\")[-1].strip('\"') if \"filename=\" in cd else f\"{pid}.zip\"\n",
    "        size = int(r.headers.get(\"content-length\", 0))\n",
    "\n",
    "        with open(f\"raw_datasets/{fname}\", \"wb\") as f, tqdm(total=size, unit=\"B\", unit_scale=True, desc=fname) as p:\n",
    "            for c in r.iter_content(chunk):\n",
    "                f.write(c); p.update(len(c))\n",
    "\n",
    "\n",
    "def get_eomt_trained_model():\n",
    "\n",
    "    # folder link\n",
    "    url = \"https://drive.google.com/drive/folders/1q2vHUzora2nP52fP50zmoQAykWuwoGav?usp=share_link\"\n",
    "\n",
    "    folder_name = \"eomt_trained_model\"\n",
    "\n",
    "    if not os.path.exists(folder_name):\n",
    "        gdown.download_folder(url, output=folder_name, quiet=True)\n",
    "\n",
    "    zip_path = os.path.join(folder_name, \"validation_Dataset.zip\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_obj:\n",
    "        for file_name in zip_obj.namelist():\n",
    "            if \"macosx\" in file_name.lower():\n",
    "                continue\n",
    "            zip_obj.extract(file_name, folder_name)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T18:38:03.605665Z",
     "start_time": "2025-12-19T18:38:03.225901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "get_datasets()\n",
    "get_eomt_trained_model()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cityscape datasets already downloaded\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T18:38:03.638535Z",
     "start_time": "2025-12-19T18:38:03.608729Z"
    }
   },
   "source": [
    "seed_everything(0, verbose=False)\n",
    "\n",
    "device = select_device(\"auto\")\n",
    "img_idx = 2  # TODO: change to the index of the image you want to visualize\n",
    "config_path = \"configs/dinov2/cityscapes/semantic/eomt_base_640.yaml\"\n",
    "data_path = \"raw_datasets\"  # dataset directory\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "def create_mapping(images, ignore_index):\n",
    "    unique_ids = np.unique(np.concatenate([np.unique(img) for img in images]))\n",
    "    valid_ids = unique_ids[unique_ids != ignore_index]\n",
    "    colors = np.array(\n",
    "        [plt.cm.hsv(i / len(valid_ids))[:3] for i in range(len(valid_ids))]\n",
    "    )\n",
    "    mapping = {cid: colors[i] for i, cid in enumerate(valid_ids)}\n",
    "    mapping[ignore_index] = np.array([0, 0, 0])\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def apply_colormap(image, mapping):\n",
    "    colored_image = np.zeros((*image.shape, 3))\n",
    "    for cid in np.unique(image):\n",
    "        colored_image[image == cid] = mapping.get(cid, [0, 0, 0])\n",
    "    return colored_image"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Ensure the dataset files are correctly prepared and placed in the folder specified by `data_path`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T18:38:04.030217Z",
     "start_time": "2025-12-19T18:38:03.641646Z"
    }
   },
   "source": [
    "data_module_name, class_name = config[\"data\"][\"class_path\"].rsplit(\".\", 1)\n",
    "data_module = getattr(importlib.import_module(data_module_name), class_name)\n",
    "data_module_kwargs = config[\"data\"].get(\"init_args\", {})\n",
    "\n",
    "data = data_module(\n",
    "    path=data_path,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    check_empty_targets=False,\n",
    "    **data_module_kwargs\n",
    ").setup()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T18:38:07.018604Z",
     "start_time": "2025-12-19T18:38:04.033329Z"
    }
   },
   "source": [
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*Attribute 'network' is an instance of `nn\\.Module` and is already saved during checkpointing.*\",\n",
    ")\n",
    "\n",
    "# Load encoder\n",
    "encoder_cfg = config[\"model\"][\"init_args\"][\"network\"][\"init_args\"][\"encoder\"]\n",
    "encoder_module_name, encoder_class_name = encoder_cfg[\"class_path\"].rsplit(\".\", 1)\n",
    "encoder_cls = getattr(importlib.import_module(encoder_module_name), encoder_class_name)\n",
    "encoder = encoder_cls(img_size=data.img_size, **encoder_cfg.get(\"init_args\", {}))\n",
    "\n",
    "# Load network\n",
    "network_cfg = config[\"model\"][\"init_args\"][\"network\"]\n",
    "network_module_name, network_class_name = network_cfg[\"class_path\"].rsplit(\".\", 1)\n",
    "network_cls = getattr(importlib.import_module(network_module_name), network_class_name)\n",
    "network_kwargs = {k: v for k, v in network_cfg[\"init_args\"].items() if k != \"encoder\"}\n",
    "network = network_cls(\n",
    "    masked_attn_enabled=False,\n",
    "    num_classes=data.num_classes,\n",
    "    encoder=encoder,\n",
    "    **network_kwargs,\n",
    ")\n",
    "\n",
    "# Load Lightning module\n",
    "lit_module_name, lit_class_name = config[\"model\"][\"class_path\"].rsplit(\".\", 1)\n",
    "lit_cls = getattr(importlib.import_module(lit_module_name), lit_class_name)\n",
    "model_kwargs = {k: v for k, v in config[\"model\"][\"init_args\"].items() if k != \"network\"}\n",
    "if \"stuff_classes\" in config[\"data\"].get(\"init_args\", {}):\n",
    "    model_kwargs[\"stuff_classes\"] = config[\"data\"][\"init_args\"][\"stuff_classes\"]\n",
    "\n",
    "model = (\n",
    "    lit_cls(\n",
    "        img_size=data.img_size,\n",
    "        num_classes=data.num_classes,\n",
    "        network=network,\n",
    "        **model_kwargs,\n",
    "    )\n",
    "    .eval()\n",
    "    .to(device)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained weights from Hugging Face Hub\n",
    "The model weights are downloaded from the Hugging Face Hub using the logger name from the config. Make sure you have a working internet connection."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T18:38:07.081346Z",
     "start_time": "2025-12-19T18:38:07.078537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_pretrained_model(model_,\n",
    "                          device_,\n",
    "                          *,\n",
    "                          config_=None,\n",
    "                          local_path=None,\n",
    "                          repo_namespace=\"tue-mps\",\n",
    "                          hf_filename=\"pytorch_model.bin\",\n",
    "                          strict=False,\n",
    "                          map_location=None,\n",
    "                          ):\n",
    "\n",
    "    ckpt_path = local_path or None\n",
    "\n",
    "    if config_:\n",
    "        name_ = (config_.get(\"trainer\", {})\n",
    "                .get(\"logger\", {})\n",
    "                .get(\"init_args\", {})\n",
    "                .get(\"name\"))\n",
    "\n",
    "        if name_:\n",
    "            try:\n",
    "                ckpt_path = hf_hub_download(\n",
    "                    repo_id=f\"{repo_namespace}/{name_}\",\n",
    "                    filename=hf_filename)\n",
    "            except RepositoryNotFoundError:\n",
    "                warnings.warn(f\"Pretrained model `{name_}` not found on HuggingFace Hub\")\n",
    "                return model_.eval().to(device_)\n",
    "\n",
    "    if not ckpt_path:\n",
    "        warnings.warn(\"No checkpoint provided or found\")\n",
    "        return model_.eval().to(device_)\n",
    "\n",
    "    ckpt = torch.load(ckpt_path, map_location=map_location)\n",
    "\n",
    "    # Lightning .ckpt\n",
    "    state_dict_ = ckpt[\"state_dict\"] if isinstance(ckpt, dict) and \"state_dict\" in ckpt else ckpt\n",
    "\n",
    "    # Drop positional embedding if shape mismatch\n",
    "    for key in (\"network.encoder.backbone.pos_embed\", \"encoder.backbone.pos_embed\"):\n",
    "        if key in state_dict_ and key in model_.state_dict():\n",
    "            if state_dict_[key].shape != model_.state_dict()[key].shape:\n",
    "                del state_dict_[key]\n",
    "\n",
    "    model_.load_state_dict(state_dict_, strict=strict)\n",
    "    model_.eval().to(device_)\n",
    "    return model_\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T18:38:07.095629Z",
     "start_time": "2025-12-19T18:38:07.093800Z"
    }
   },
   "source": [
    "# MAP_LOCATION = select_device(\"auto\")\n",
    "# name = config.get(\"trainer\", {}).get(\"logger\", {}).get(\"init_args\", {}).get(\"name\")\n",
    "#\n",
    "# if name is None:\n",
    "#     warnings.warn(\"No logger name found in the config. Please specify a model name.\")\n",
    "# else:\n",
    "#     try:\n",
    "#         state_dict_path = hf_hub_download(\n",
    "#             repo_id=f\"tue-mps/{name}\",\n",
    "#             filename=\"pytorch_model.bin\",\n",
    "#         )\n",
    "#\n",
    "#         if is_dinov3 := \"dinov3\" in name:\n",
    "#             model_kwargs[\"ckpt_path\"] = state_dict_path\n",
    "#             model_kwargs[\"delta_weights\"] = True\n",
    "#\n",
    "#         model = (\n",
    "#             lit_cls(\n",
    "#                 img_size=data.img_size,\n",
    "#                 num_classes=data.num_classes,\n",
    "#                 network=network,\n",
    "#                 **model_kwargs,\n",
    "#             )\n",
    "#             .eval()\n",
    "#             .to(device)\n",
    "#         )\n",
    "#\n",
    "#         if not is_dinov3:\n",
    "#             state_dict = torch.load(state_dict_path, map_location=MAP_LOCATION, weights_only=True)\n",
    "#             model.load_state_dict(state_dict, strict=False)\n",
    "#\n",
    "#     except RepositoryNotFoundError:\n",
    "#         warnings.warn(f\"Pre-trained model not found for `{name}`. Please load your own checkpoint.\")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T18:38:07.541638Z",
     "start_time": "2025-12-19T18:38:07.105203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# eomt (via config)\n",
    "# model = load_pretrained_model(\n",
    "#     model,\n",
    "#     device,\n",
    "#     config_=config,\n",
    "#     repo_namespace=\"tue-mps\",\n",
    "#     hf_filename=\"pytorch_model.bin\",\n",
    "#     map_location=device,\n",
    "#     strict=False)\n",
    "\n",
    "# eomt (da .ckpt locale)\n",
    "model = load_pretrained_model(\n",
    "    model,\n",
    "    device,\n",
    "    local_path=\"eomt_trained_model/epoch_106-step_19902_eomt.ckpt\",\n",
    "    map_location=device,\n",
    "    strict=False)\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Semantic inference (pixel-wise classification)\n",
    "\n",
    "> This inference method also works when applied to a model trained for panoptic segmentation.\n",
    "\n",
    "Semantic inference computes per-pixel class scores by combining mask and class predictions:\n",
    "\n",
    "$$\n",
    "\\sum_i p_i(c) \\cdot m_i[h, w]\n",
    "$$\n",
    "\n",
    "Here, $p_i(c)$ is the class probability for class $c$ (excluding \"no object\"), and $m_i[h, w]$ is the sigmoid-normalized mask value for query $i$ at pixel $(h, w)$. The final class is selected by taking the argmax over classes.\n",
    "\n",
    "*This inference method was originally introduced in MaskFormer.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T18:38:07.548306Z",
     "start_time": "2025-12-19T18:38:07.544809Z"
    }
   },
   "source": [
    "IGNORE_INDEX = 255\n",
    "DEVICE_TYPE = select_device(\"auto\").type\n",
    "\n",
    "def infer_semantic(img, target):\n",
    "    with torch.no_grad(), autocast(dtype=torch.float16, device_type=DEVICE_TYPE):\n",
    "        imgs = [img.to(device)]\n",
    "        img_sizes = [img.shape[-2:] for img in imgs]\n",
    "        crops, origins = model.window_imgs_semantic(imgs)\n",
    "\n",
    "        mask_logits_per_layer, class_logits_per_layer = model(crops)\n",
    "        mask_logits = F.interpolate(\n",
    "            mask_logits_per_layer[-1], data.img_size, mode=\"bilinear\"\n",
    "        )\n",
    "\n",
    "        crop_logits = model.to_per_pixel_logits_semantic(\n",
    "            mask_logits, class_logits_per_layer[-1]\n",
    "        )\n",
    "        logits = model.revert_window_logits_semantic(crop_logits, origins, img_sizes)\n",
    "        preds = logits[0].argmax(0).cpu()\n",
    "\n",
    "    pred_array = preds.numpy()\n",
    "    target_array = model.to_per_pixel_targets_semantic([target], IGNORE_INDEX)[0].numpy()\n",
    "    return logits[0], pred_array, target_array\n",
    "\n",
    "\n",
    "def plot_semantic_results(img, pred_array, target_array):\n",
    "    mapping = create_mapping([pred_array, target_array], IGNORE_INDEX)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5), tight_layout=True)\n",
    "    axes[0].imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "    axes[0].set_title(\"Image\")\n",
    "    axes[1].imshow(apply_colormap(pred_array, mapping))\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "    axes[2].imshow(apply_colormap(target_array, mapping))\n",
    "    axes[2].set_title(\"Target\")\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# img, target = data.val_dataloader().dataset[img_idx]\n",
    "# pred_array, target_array = infer_semantic(img, target)\n",
    "# plot_semantic_results(img, pred_array, target_array)\n",
    "\n",
    "# res = {\"logits\": [], \"target\": []}\n",
    "# datasets = data.val_dataloader().dataset\n",
    "#\n",
    "# for img, target in tqdm(datasets,\n",
    "#                             total=len(datasets),\n",
    "#                             desc=\"Validation\",\n",
    "#                             ncols=600):\n",
    "#         logit, pred_array, target_array = infer_semantic(img, target)\n",
    "#\n",
    "#         res[\"logits\"].append(logit)\n",
    "#         res[\"target\"].append(target)\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T18:38:07.562670Z",
     "start_time": "2025-12-19T18:38:07.556186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_anomaly_maps(logits_chw: torch.Tensor, T: float) -> dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute pixel-wise anomaly score maps from logits [C,H,W] for a given temperature T\n",
    "    \"\"\"\n",
    "\n",
    "    z = logits_chw / T\n",
    "    probs = F.softmax(z, dim=0)\n",
    "\n",
    "    # MSP\n",
    "    msp = 1. - probs.max(dim=0).values\n",
    "\n",
    "    # MaxLogit\n",
    "    maxlogit = -z.max(dim=0).values\n",
    "\n",
    "    # Entropy\n",
    "    eps = 1e-12\n",
    "    entropy = -torch.sum(probs * torch.log(probs + eps), dim=0)\n",
    "\n",
    "    return {\"MSP\": msp, \"MaxLogit\": maxlogit, \"Entropy\": entropy}\n",
    "\n",
    "\n",
    "def eval_one_dataset(\n",
    "        *,\n",
    "        dataset_name: str,\n",
    "        dataloader: torch.utils.data.DataLoader,\n",
    "        Ts: Iterable[float],\n",
    "        device_: torch.device,\n",
    "        ignore_index: int = IGNORE_INDEX) -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Run semantic inference and compute pixel-wise  AuPRC and FPR@95TPR for MSP/MaxLogit/Entropy\n",
    "    \"\"\"\n",
    "    methods = (\"MSP\", \"MaxLogit\", \"Entropy\")\n",
    "\n",
    "    acc_labels: list[np.ndarray] = []\n",
    "    acc_scores: dict[tuple[str, float], list[np.ndarray]] = {(m, t): [] for m in methods for t in Ts}\n",
    "\n",
    "    for _img, _target in tqdm(dataloader, desc=f\"Evaluating {dataset_name:>20s}\"):\n",
    "        if isinstance(_img, torch.Tensor) and _img.dim() == 4:\n",
    "            _img = _img[0]\n",
    "\n",
    "        if isinstance(_target, dict):\n",
    "            _target = {k:(v[0] if isinstance(v, torch.Tensor) else v) for k,v in _target.items()}\n",
    "        elif isinstance(_target, torch.Tensor) and _target.dim() == 3:\n",
    "            _target = _target[0]\n",
    "\n",
    "        logits_chw, _, target_array = infer_semantic(_img, _target)\n",
    "        logits_cpu = logits_chw.detach().to(\"cpu\", dtype=torch.float32)\n",
    "        gt = target_array\n",
    "        valid = (gt != ignore_index)\n",
    "\n",
    "        if not np.any(valid): continue\n",
    "\n",
    "        # binarize labels\n",
    "        y = (gt[valid] == 1).astype(np.uint8)\n",
    "        acc_labels.append(y)\n",
    "\n",
    "        # compute a score map for each T and store only valid pixels\n",
    "        for t in Ts:\n",
    "            maps = compute_anomaly_maps(logits_cpu, t)\n",
    "            for m in methods:\n",
    "                score = maps[m].numpy()[valid].astype(np.float32)\n",
    "                acc_scores[(m,t)].append(score)\n",
    "\n",
    "    if not len(acc_labels):\n",
    "        warnings.warn(f\"No valid pixels found for dataset `{dataset_name}`\")\n",
    "        return []\n",
    "\n",
    "    y_true = np.concatenate(acc_labels, axis=0)\n",
    "\n",
    "    rows: list[dict[str, Any]] = []\n",
    "    for t in Ts:\n",
    "        for m in methods:\n",
    "            scores = np.concatenate(acc_scores[(m,t)], axis=0)\n",
    "            auprc = round(average_precision_score(y_true, scores) * 100., 2)\n",
    "            fpr95 = round(fpr_at_95_tpr(scores, y_true) * 100., 2)\n",
    "\n",
    "            rows.append({\n",
    "               \"dataset\": dataset_name,\n",
    "               \"method\": m,\n",
    "               \"T\": t,\n",
    "               \"AuPRC\": auprc,\n",
    "               \"FPR95\": fpr95})\n",
    "\n",
    "    return rows\n",
    "\n",
    "def run_all_datasets(\n",
    "        *,\n",
    "        device_: torch.device,\n",
    "        dataset_cfg: list[dict[str, Any]],\n",
    "        Ts: Iterable[float],\n",
    "        img_size: tuple[int, int],\n",
    "        batch_size: int,\n",
    "        num_workers: int,\n",
    "        out_csv: str) -> str:\n",
    "    \"\"\"Iterate over dataset config, run, eval and save a single CSV with all results\"\"\"\n",
    "\n",
    "    all_rows: list[dict[str, Any]] = []\n",
    "\n",
    "    for ds_cfg in dataset_cfg:\n",
    "        ds_name = ds_cfg['name']\n",
    "        ds = GenericAnomalyDataset(ds_cfg, img_size=img_size)\n",
    "        dl = torch.utils.data.DataLoader(ds,\n",
    "                                         batch_size=batch_size,\n",
    "                                         num_workers=num_workers)\n",
    "\n",
    "        rows = eval_one_dataset(\n",
    "            dataset_name=ds_name,\n",
    "            dataloader=dl,\n",
    "            Ts=Ts,\n",
    "            device_=device_,\n",
    "            ignore_index=IGNORE_INDEX)\n",
    "\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "    with open(out_csv, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"dataset\", \"method\", \"T\", \"AuPRC\", \"FPR95\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_rows)\n",
    "\n",
    "    return out_csv"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T18:45:09.293111Z",
     "start_time": "2025-12-19T18:38:07.570409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_table5_csv():\n",
    "    # Table 5: same temperature for all methods\n",
    "    Ts = [1]\n",
    "\n",
    "    anomaly_config_path = \"configs/dinov2/common/generic_anomaly.yaml\"\n",
    "\n",
    "    with open(anomaly_config_path, \"r\") as f:\n",
    "        anomaly_cfg = yaml.safe_load(f)\n",
    "\n",
    "    datasets_cfg = anomaly_cfg[\"data\"][\"init_args\"][\"datasets\"]\n",
    "    img_size = tuple(anomaly_cfg[\"data\"][\"init_args\"][\"img_size\"])\n",
    "    num_workers = int(anomaly_cfg[\"data\"][\"init_args\"][\"num_workers\"])\n",
    "    out_csv = \"results/table5.csv\"\n",
    "\n",
    "    out_csv = run_all_datasets(\n",
    "        device_=device,\n",
    "        dataset_cfg=datasets_cfg,\n",
    "        Ts=Ts,\n",
    "        img_size=img_size,\n",
    "        batch_size=1,\n",
    "        num_workers=num_workers,\n",
    "        out_csv=out_csv)\n",
    "\n",
    "get_table5_csv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluating          RoadAnomaly:   0%|          | 0/60 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8de98718fdeb4c97a1837c7416bb5a8e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Evaluating        RoadAnomaly21:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53d2271778e34ab4831142de07c4e3db"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Evaluating       RoadObstacle21:   0%|          | 0/30 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c25ab19fc5d641cca6bfcdcecc504512"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Evaluating    FS_LostFound_full:   0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "533b9f84d31f4bc88e166eacf4c74105"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Evaluating            FS_Static:   0%|          | 0/30 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1ad4ba4c64542218d7431ca638809ee"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T19:11:28.529763Z",
     "start_time": "2025-12-19T18:45:09.320006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_table6_csv():\n",
    "    # Table 5: same temperature for all methods\n",
    "    Ts = [0.25, 0.33, 0.5, 0.67, 0.75, 1.0, 1.25, 1.5, 2.0, 3.0, 4.0, 5.0, 8.0]\n",
    "\n",
    "    anomaly_config_path = \"configs/dinov2/common/generic_anomaly.yaml\"\n",
    "\n",
    "    with open(anomaly_config_path, \"r\") as f:\n",
    "        anomaly_cfg = yaml.safe_load(f)\n",
    "\n",
    "    datasets_cfg = anomaly_cfg[\"data\"][\"init_args\"][\"datasets\"]\n",
    "    img_size = tuple(anomaly_cfg[\"data\"][\"init_args\"][\"img_size\"])\n",
    "    num_workers = int(anomaly_cfg[\"data\"][\"init_args\"][\"num_workers\"])\n",
    "    out_csv = \"results/table6.csv\"\n",
    "\n",
    "    out_csv = run_all_datasets(\n",
    "        device_=device,\n",
    "        dataset_cfg=datasets_cfg,\n",
    "        Ts=Ts,\n",
    "        img_size=img_size,\n",
    "        batch_size=1,\n",
    "        num_workers=num_workers,\n",
    "        out_csv=out_csv)\n",
    "\n",
    "get_table6_csv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluating          RoadAnomaly:   0%|          | 0/60 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1553bf9d557466391c59b1f5e75f2dc"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evaluating        RoadAnomaly21:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8579ad536c9a410a979cc88d44faeb4d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Evaluating       RoadObstacle21:   0%|          | 0/30 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fcf5c72e26b4ee590ca6e315aa3bd37"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Evaluating    FS_LostFound_full:   0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7094bc7108204b1cb439a03e184b921b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Evaluating            FS_Static:   0%|          | 0/30 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "761f4e429bb2426a85ab9863aaac3dff"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panoptic inference (segmentation with instance IDs)\n",
    "\n",
    "> This inference method also works when applied to a model trained for instance segmentation.\n",
    "\n",
    "Panoptic inference assigns each pixel $[h, w]$ to the query $i$ that maximizes the product of class and mask confidence:\n",
    "\n",
    "$$\n",
    "p_i(c_i) \\cdot m_i[h, w]\n",
    "$$\n",
    "\n",
    "where $c_i = \\arg\\max_c \\, p_i(c)$ is the most likely class for query $i$. A pixel is assigned to a query only if both the class confidence and mask confidence are high. Pixels assigned to the same query form a segment labeled with $c_i$. \"Stuff\" segments with the same class are merged; \"thing\" segments are kept distinct using the query index. Low-confidence and heavily occluded predictions are filtered out.  \n",
    "  \n",
    "*This inference method was originally introduced in MaskFormer.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T19:11:28.557839Z",
     "start_time": "2025-12-19T19:11:28.555488Z"
    }
   },
   "source": [
    "# def infer_panoptic(img, target):\n",
    "#     with torch.no_grad(), autocast(dtype=torch.float16, device_type=device):\n",
    "#         imgs = [img.to(device)]\n",
    "#         img_sizes = [img.shape[-2:] for img in imgs]\n",
    "#\n",
    "#         transformed_imgs = model.resize_and_pad_imgs_instance_panoptic(imgs)\n",
    "#         mask_logits_per_layer, class_logits_per_layer = model(transformed_imgs)\n",
    "#         mask_logits = F.interpolate(\n",
    "#             mask_logits_per_layer[-1], model.img_size, mode=\"bilinear\"\n",
    "#         )\n",
    "#         mask_logits = model.revert_resize_and_pad_logits_instance_panoptic(\n",
    "#             mask_logits, img_sizes\n",
    "#         )\n",
    "#\n",
    "#         preds = model.to_per_pixel_preds_panoptic(\n",
    "#             mask_logits,\n",
    "#             class_logits_per_layer[-1],\n",
    "#             model.stuff_classes,\n",
    "#             model.mask_thresh,\n",
    "#             model.overlap_thresh,\n",
    "#         )[0].cpu()\n",
    "#\n",
    "#     pred = preds.numpy()\n",
    "#     sem_pred, inst_pred = pred[..., 0], pred[..., 1]\n",
    "#\n",
    "#     target_seg = model.to_per_pixel_targets_panoptic([target])[0].cpu().numpy()\n",
    "#     sem_target, inst_target = target_seg[..., 0], target_seg[..., 1]\n",
    "#\n",
    "#     return sem_pred, inst_pred, sem_target, inst_target\n",
    "#\n",
    "#\n",
    "# def draw_black_border(sem, inst, mapping):\n",
    "#     h, w = sem.shape\n",
    "#     out = np.zeros((h, w, 3))\n",
    "#     for s in np.unique(sem):\n",
    "#         out[sem == s] = mapping[s]\n",
    "#\n",
    "#     combined = sem.astype(np.int64) * 100000 + inst.astype(np.int64)\n",
    "#     border = np.zeros((h, w), dtype=bool)\n",
    "#     border[1:, :] |= combined[1:, :] != combined[:-1, :]\n",
    "#     border[:-1, :] |= combined[1:, :] != combined[:-1, :]\n",
    "#     border[:, 1:] |= combined[:, 1:] != combined[:, :-1]\n",
    "#     border[:, :-1] |= combined[:, 1:] != combined[:, :-1]\n",
    "#     out[border] = 0\n",
    "#     return out\n",
    "#\n",
    "#\n",
    "# def plot_panoptic_results(img, sem_pred, inst_pred, sem_target, inst_target):\n",
    "#     all_ids = np.union1d(np.unique(sem_pred), np.unique(sem_target))\n",
    "#     mapping = {\n",
    "#         s: (\n",
    "#             [0, 0, 0]\n",
    "#             if s == -1 or s == model.num_classes\n",
    "#             else plt.cm.hsv(i / len(all_ids))[:3]\n",
    "#         )\n",
    "#         for i, s in enumerate(all_ids)\n",
    "#     }\n",
    "#\n",
    "#     vis_pred = draw_black_border(sem_pred, inst_pred, mapping)\n",
    "#     vis_target = draw_black_border(sem_target, inst_target, mapping)\n",
    "#\n",
    "#     img_np = (\n",
    "#         img.cpu().numpy().transpose(1, 2, 0) if img.dim() == 3 else img.cpu().numpy()\n",
    "#     )\n",
    "#\n",
    "#     fig, axes = plt.subplots(1, 3, figsize=(15, 5), tight_layout=True)\n",
    "#     axes[0].imshow(img_np)\n",
    "#     axes[0].set_title(\"Input\")\n",
    "#     axes[1].imshow(vis_pred)\n",
    "#     axes[1].set_title(\"Prediction\")\n",
    "#     axes[2].imshow(vis_target)\n",
    "#     axes[2].set_title(\"Target\")\n",
    "#\n",
    "#     for ax in axes:\n",
    "#         ax.axis(\"off\")\n",
    "#\n",
    "#     plt.show()\n",
    "#\n",
    "#\n",
    "# img, target = data.val_dataloader().dataset[img_idx]\n",
    "# sem_pred, inst_pred, sem_target, inst_target = infer_panoptic(img, target)\n",
    "# plot_panoptic_results(img, sem_pred, inst_pred, sem_target, inst_target)"
   ],
   "outputs": [],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eomt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
