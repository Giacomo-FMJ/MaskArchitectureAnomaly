{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© 2025 Mobile Perception Systems Lab at TU/e. All rights reserved. Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import warnings\n",
    "import requests\n",
    "import importlib\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.amp.autocast_mode import autocast\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub.utils import RepositoryNotFoundError\n",
    "\n",
    "import lightning\n",
    "from lightning import seed_everything"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_datasets(datapath: str = \"\"):\n",
    "\n",
    "    chunk = 1024 ** 2 # 1 MB chunk\n",
    "    datapath = datapath or \"raw_datasets\"\n",
    "    os.makedirs(datapath, exist_ok=True)\n",
    "\n",
    "    if len(os.listdir(datapath)):\n",
    "        print(\"Dataset already downloaded\")\n",
    "        return\n",
    "\n",
    "    s = requests.Session()\n",
    "    s.post(\"https://www.cityscapes-dataset.com/login/\",\n",
    "           data={\"username\":\"AdryG\",\"password\":\"Agf2gc262!\",\"submit\":\"Login\"})\n",
    "\n",
    "    for pid in [1, 3]:\n",
    "        r = s.get(f\"https://www.cityscapes-dataset.com/file-handling/?packageID={pid}\",\n",
    "                  stream=True, allow_redirects=True)\n",
    "\n",
    "        r.raise_for_status()\n",
    "\n",
    "        cd = r.headers.get(\"content-disposition\",\"\")\n",
    "        fname = cd.split(\"filename=\")[-1].strip('\"') if \"filename=\" in cd else f\"{pid}.zip\"\n",
    "        size = int(r.headers.get(\"content-length\", 0))\n",
    "\n",
    "        with open(f\"raw_datasets/{fname}\", \"wb\") as f, tqdm(total=size, unit=\"B\", unit_scale=True, desc=fname) as p:\n",
    "            for c in r.iter_content(chunk):\n",
    "                f.write(c); p.update(len(c))\n",
    "\n",
    "def save_data(fname: str, data):\n",
    "\n",
    "    with open(fname, \"w\") as fout:\n",
    "        json.dump(data, fout)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_datasets()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "seed_everything(0, verbose=False)\n",
    "\n",
    "device = \"mps\" if torch.mps.is_available() else \"cpu\" # TODO: change to the GPU you want to use\n",
    "img_idx = 2  # TODO: change to the index of the image you want to visualize\n",
    "config_path = \"configs/dinov2/cityscapes/semantic/eomt_large_1024.yaml\"\n",
    "data_path = \"raw_datasets\"  # TODO: change to the dataset directory\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "def create_mapping(images, ignore_index):\n",
    "    unique_ids = np.unique(np.concatenate([np.unique(img) for img in images]))\n",
    "    valid_ids = unique_ids[unique_ids != ignore_index]\n",
    "    colors = np.array(\n",
    "        [plt.cm.hsv(i / len(valid_ids))[:3] for i in range(len(valid_ids))]\n",
    "    )\n",
    "    mapping = {cid: colors[i] for i, cid in enumerate(valid_ids)}\n",
    "    mapping[ignore_index] = np.array([0, 0, 0])\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def apply_colormap(image, mapping):\n",
    "    colored_image = np.zeros((*image.shape, 3))\n",
    "    for cid in np.unique(image):\n",
    "        colored_image[image == cid] = mapping.get(cid, [0, 0, 0])\n",
    "    return colored_image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Ensure the dataset files are correctly prepared and placed in the folder specified by `data_path`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data_module_name, class_name = config[\"data\"][\"class_path\"].rsplit(\".\", 1)\n",
    "data_module = getattr(importlib.import_module(data_module_name), class_name)\n",
    "data_module_kwargs = config[\"data\"].get(\"init_args\", {})\n",
    "\n",
    "data = data_module(\n",
    "    path=data_path,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    check_empty_targets=False,\n",
    "    **data_module_kwargs\n",
    ").setup()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*Attribute 'network' is an instance of `nn\\.Module` and is already saved during checkpointing.*\",\n",
    ")\n",
    "\n",
    "# Load encoder\n",
    "encoder_cfg = config[\"model\"][\"init_args\"][\"network\"][\"init_args\"][\"encoder\"]\n",
    "encoder_module_name, encoder_class_name = encoder_cfg[\"class_path\"].rsplit(\".\", 1)\n",
    "encoder_cls = getattr(importlib.import_module(encoder_module_name), encoder_class_name)\n",
    "encoder = encoder_cls(img_size=data.img_size, **encoder_cfg.get(\"init_args\", {}))\n",
    "\n",
    "# Load network\n",
    "network_cfg = config[\"model\"][\"init_args\"][\"network\"]\n",
    "network_module_name, network_class_name = network_cfg[\"class_path\"].rsplit(\".\", 1)\n",
    "network_cls = getattr(importlib.import_module(network_module_name), network_class_name)\n",
    "network_kwargs = {k: v for k, v in network_cfg[\"init_args\"].items() if k != \"encoder\"}\n",
    "network = network_cls(\n",
    "    masked_attn_enabled=False,\n",
    "    num_classes=data.num_classes,\n",
    "    encoder=encoder,\n",
    "    **network_kwargs,\n",
    ")\n",
    "\n",
    "# Load Lightning module\n",
    "lit_module_name, lit_class_name = config[\"model\"][\"class_path\"].rsplit(\".\", 1)\n",
    "lit_cls = getattr(importlib.import_module(lit_module_name), lit_class_name)\n",
    "model_kwargs = {k: v for k, v in config[\"model\"][\"init_args\"].items() if k != \"network\"}\n",
    "if \"stuff_classes\" in config[\"data\"].get(\"init_args\", {}):\n",
    "    model_kwargs[\"stuff_classes\"] = config[\"data\"][\"init_args\"][\"stuff_classes\"]\n",
    "\n",
    "model = (\n",
    "    lit_cls(\n",
    "        img_size=data.img_size,\n",
    "        num_classes=data.num_classes,\n",
    "        network=network,\n",
    "        **model_kwargs,\n",
    "    )\n",
    "    .eval()\n",
    "    .to(device)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained weights from Hugging Face Hub\n",
    "The model weights are downloaded from the Hugging Face Hub using the logger name from the config. Make sure you have a working internet connection."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "MAP_LOCATION = device if torch.mps.is_available() else  f\"cuda:{device}\"\n",
    "name = config.get(\"trainer\", {}).get(\"logger\", {}).get(\"init_args\", {}).get(\"name\")\n",
    "\n",
    "if name is None:\n",
    "    warnings.warn(\"No logger name found in the config. Please specify a model name.\")\n",
    "else:\n",
    "    try:\n",
    "        state_dict_path = hf_hub_download(\n",
    "            repo_id=f\"tue-mps/{name}\",\n",
    "            filename=\"pytorch_model.bin\",\n",
    "        )\n",
    "\n",
    "        is_dinov3 = \"dinov3\" in name\n",
    "\n",
    "        if is_dinov3:\n",
    "            model_kwargs[\"ckpt_path\"] = state_dict_path\n",
    "            model_kwargs[\"delta_weights\"] = True\n",
    "\n",
    "        model = (\n",
    "            lit_cls(\n",
    "                img_size=data.img_size,\n",
    "                num_classes=data.num_classes,\n",
    "                network=network,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "            .eval()\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        if not is_dinov3:\n",
    "            state_dict = torch.load(state_dict_path, map_location=MAP_LOCATION, weights_only=True)\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    except RepositoryNotFoundError:\n",
    "        warnings.warn(\n",
    "            f\"Pre-trained model not found for `{name}`. Please load your own checkpoint.\"\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Semantic inference (pixel-wise classification)\n",
    "\n",
    "> This inference method also works when applied to a model trained for panoptic segmentation.\n",
    "\n",
    "Semantic inference computes per-pixel class scores by combining mask and class predictions:\n",
    "\n",
    "$$\n",
    "\\sum_i p_i(c) \\cdot m_i[h, w]\n",
    "$$\n",
    "\n",
    "Here, $p_i(c)$ is the class probability for class $c$ (excluding \"no object\"), and $m_i[h, w]$ is the sigmoid-normalized mask value for query $i$ at pixel $(h, w)$. The final class is selected by taking the argmax over classes.\n",
    "\n",
    "*This inference method was originally introduced in MaskFormer.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T09:49:56.612898Z",
     "start_time": "2025-12-18T09:49:53.433420Z"
    }
   },
   "source": [
    "IGNORE_INDEX = 255\n",
    "DEVICE_TYPE = \"mps\"\n",
    "\n",
    "def infer_semantic(img, target):\n",
    "    with torch.no_grad(), autocast(dtype=torch.float16, device_type=DEVICE_TYPE):\n",
    "        imgs = [img.to(device)]\n",
    "        img_sizes = [img.shape[-2:] for img in imgs]\n",
    "        crops, origins = model.window_imgs_semantic(imgs)\n",
    "\n",
    "        mask_logits_per_layer, class_logits_per_layer = model(crops)\n",
    "        mask_logits = F.interpolate(\n",
    "            mask_logits_per_layer[-1], data.img_size, mode=\"bilinear\"\n",
    "        )\n",
    "\n",
    "        crop_logits = model.to_per_pixel_logits_semantic(\n",
    "            mask_logits, class_logits_per_layer[-1]\n",
    "        )\n",
    "        logits = model.revert_window_logits_semantic(crop_logits, origins, img_sizes)\n",
    "        preds = logits[0].argmax(0).cpu()\n",
    "\n",
    "    pred_array = preds.numpy()\n",
    "    target_array = model.to_per_pixel_targets_semantic([target], IGNORE_INDEX)[0].numpy()\n",
    "    return logits[0], pred_array, target_array\n",
    "\n",
    "\n",
    "def plot_semantic_results(img, pred_array, target_array):\n",
    "    mapping = create_mapping([pred_array, target_array], IGNORE_INDEX)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5), tight_layout=True)\n",
    "    axes[0].imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "    axes[0].set_title(\"Image\")\n",
    "    axes[1].imshow(apply_colormap(pred_array, mapping))\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "    axes[2].imshow(apply_colormap(target_array, mapping))\n",
    "    axes[2].set_title(\"Target\")\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# img, target = data.val_dataloader().dataset[img_idx]\n",
    "# pred_array, target_array = infer_semantic(img, target)\n",
    "# plot_semantic_results(img, pred_array, target_array)\n",
    "\n",
    "res = {\"logits\": [], \"target\": []}\n",
    "datasets = data.val_dataloader().dataset\n",
    "\n",
    "for img, target in tqdm(datasets,\n",
    "                            total=len(datasets),\n",
    "                            desc=\"Validation\",\n",
    "                            ncols=600):\n",
    "        logit, pred_array, target_array = infer_semantic(img, target)\n",
    "\n",
    "        res[\"logits\"].append(logit)\n",
    "        res[\"target\"].append(target)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Validation:   0%|                                                                                             …"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7cf77d5c6134337bd49f28e28c83ae0"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19, 1024, 2048]) torch.float32 (1024, 2048) (1024, 2048)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panoptic inference (segmentation with instance IDs)\n",
    "\n",
    "> This inference method also works when applied to a model trained for instance segmentation.\n",
    "\n",
    "Panoptic inference assigns each pixel $[h, w]$ to the query $i$ that maximizes the product of class and mask confidence:\n",
    "\n",
    "$$\n",
    "p_i(c_i) \\cdot m_i[h, w]\n",
    "$$\n",
    "\n",
    "where $c_i = \\arg\\max_c \\, p_i(c)$ is the most likely class for query $i$. A pixel is assigned to a query only if both the class confidence and mask confidence are high. Pixels assigned to the same query form a segment labeled with $c_i$. \"Stuff\" segments with the same class are merged; \"thing\" segments are kept distinct using the query index. Low-confidence and heavily occluded predictions are filtered out.  \n",
    "  \n",
    "*This inference method was originally introduced in MaskFormer.*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def infer_panoptic(img, target):\n",
    "    with torch.no_grad(), autocast(dtype=torch.float16, device_type=device):\n",
    "        imgs = [img.to(device)]\n",
    "        img_sizes = [img.shape[-2:] for img in imgs]\n",
    "\n",
    "        transformed_imgs = model.resize_and_pad_imgs_instance_panoptic(imgs)\n",
    "        mask_logits_per_layer, class_logits_per_layer = model(transformed_imgs)\n",
    "        mask_logits = F.interpolate(\n",
    "            mask_logits_per_layer[-1], model.img_size, mode=\"bilinear\"\n",
    "        )\n",
    "        mask_logits = model.revert_resize_and_pad_logits_instance_panoptic(\n",
    "            mask_logits, img_sizes\n",
    "        )\n",
    "\n",
    "        preds = model.to_per_pixel_preds_panoptic(\n",
    "            mask_logits,\n",
    "            class_logits_per_layer[-1],\n",
    "            model.stuff_classes,\n",
    "            model.mask_thresh,\n",
    "            model.overlap_thresh,\n",
    "        )[0].cpu()\n",
    "\n",
    "    pred = preds.numpy()\n",
    "    sem_pred, inst_pred = pred[..., 0], pred[..., 1]\n",
    "\n",
    "    target_seg = model.to_per_pixel_targets_panoptic([target])[0].cpu().numpy()\n",
    "    sem_target, inst_target = target_seg[..., 0], target_seg[..., 1]\n",
    "\n",
    "    return sem_pred, inst_pred, sem_target, inst_target\n",
    "\n",
    "\n",
    "def draw_black_border(sem, inst, mapping):\n",
    "    h, w = sem.shape\n",
    "    out = np.zeros((h, w, 3))\n",
    "    for s in np.unique(sem):\n",
    "        out[sem == s] = mapping[s]\n",
    "\n",
    "    combined = sem.astype(np.int64) * 100000 + inst.astype(np.int64)\n",
    "    border = np.zeros((h, w), dtype=bool)\n",
    "    border[1:, :] |= combined[1:, :] != combined[:-1, :]\n",
    "    border[:-1, :] |= combined[1:, :] != combined[:-1, :]\n",
    "    border[:, 1:] |= combined[:, 1:] != combined[:, :-1]\n",
    "    border[:, :-1] |= combined[:, 1:] != combined[:, :-1]\n",
    "    out[border] = 0\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_panoptic_results(img, sem_pred, inst_pred, sem_target, inst_target):\n",
    "    all_ids = np.union1d(np.unique(sem_pred), np.unique(sem_target))\n",
    "    mapping = {\n",
    "        s: (\n",
    "            [0, 0, 0]\n",
    "            if s == -1 or s == model.num_classes\n",
    "            else plt.cm.hsv(i / len(all_ids))[:3]\n",
    "        )\n",
    "        for i, s in enumerate(all_ids)\n",
    "    }\n",
    "\n",
    "    vis_pred = draw_black_border(sem_pred, inst_pred, mapping)\n",
    "    vis_target = draw_black_border(sem_target, inst_target, mapping)\n",
    "\n",
    "    img_np = (\n",
    "        img.cpu().numpy().transpose(1, 2, 0) if img.dim() == 3 else img.cpu().numpy()\n",
    "    )\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].imshow(img_np)\n",
    "    axes[0].set_title(\"Input\")\n",
    "    axes[1].imshow(vis_pred)\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "    axes[2].imshow(vis_target)\n",
    "    axes[2].set_title(\"Target\")\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "img, target = data.val_dataloader().dataset[img_idx]\n",
    "sem_pred, inst_pred, sem_target, inst_target = infer_panoptic(img, target)\n",
    "plot_panoptic_results(img, sem_pred, inst_pred, sem_target, inst_target)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eomt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
